{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"024_wt1_450mA_u18gap10p6_Au26um_orca_2x_3p075um_d1ts1037_50ms_5400proj.nxs\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = r\"D:\\Soleil\\024_wt1_450mA_u18gap10p6_Au26um_orca_2x_3p075um_d1ts1037_50ms_5400proj\\024_wt1_450mA_u18gap10p6_Au26um_orca_2x_3p075um_d1ts1037_50ms_5400proj.nxs\"\n",
    "\n",
    "\n",
    " \n",
    "with h5py.File(file, \"r\") as f:\n",
    "    # Verify dataset structure\n",
    "    print(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture du fichier HDF5 :\n",
      "[Group] flyscan_0001\n",
      "  [Group] ANATOMIX\n",
      "    [Group] ans-c13-ei-l-u18.atx\n",
      "      [Dataset] controller_record - shape: (), dtype: |S20\n",
      "      [Dataset] gap - shape: (1,), dtype: float64\n",
      "      [Dataset] gap_velocity - shape: (1,), dtype: float64\n",
      "    [Group] ans-ca-machinestatus\n",
      "      [Dataset] current - shape: (1,), dtype: float64\n",
      "      [Dataset] function_mode - shape: (), dtype: |S6\n",
      "      [Dataset] life_time - shape: (1,), dtype: float64\n",
      "      [Dataset] name - shape: (), dtype: |S6\n",
      "      [Dataset] probe - shape: (), dtype: |S5\n",
      "      [Dataset] type - shape: (), dtype: |S24\n",
      "    [Group] i13-lt-c01-ex-fent_h\n",
      "      [Dataset] device_name - shape: (), dtype: |S20\n",
      "      [Dataset] gap - shape: (1,), dtype: float64\n",
      "      [Dataset] inside_up_position - shape: (1,), dtype: float64\n",
      "      [Dataset] outside_down_position - shape: (1,), dtype: float64\n",
      "      [Dataset] position - shape: (1,), dtype: float64\n",
      "      [Dataset] position_offset - shape: (1,), dtype: float64\n",
      "    [Group] i13-lt-c01-ex-fent_h-mt_i\n",
      "      [Dataset] controller_record - shape: (), dtype: |S25\n",
      "      [Dataset] offset - shape: (1,), dtype: float64\n",
      "      [Dataset] raw_value - shape: (1,), dtype: float64\n",
      "    [Group] i13-lt-c01-ex-fent_h-mt_o\n",
      "      [Dataset] controller_record - shape: (), dtype: |S25\n",
      "      [Dataset] offset - shape: (1,), dtype: float64\n",
      "      [Dataset] raw_value - shape: (1,), dtype: float64\n",
      "    [Group] i13-lt-c01-ex-fent_v\n",
      "      [Dataset] device_name - shape: (), dtype: |S20\n",
      "      [Dataset] gap - shape: (1,), dtype: float64\n",
      "      [Dataset] inside_up_position - shape: (1,), dtype: float64\n",
      "      [Dataset] outside_down_position - shape: (1,), dtype: float64\n",
      "      [Dataset] position - shape: (1,), dtype: float64\n",
      "      [Dataset] position_offset - shape: (1,), dtype: float64\n",
      "    [Group] i13-lt-c01-ex-fent_v-mt_d\n",
      "      [Dataset] controller_record - shape: (), dtype: |S25\n",
      "      [Dataset] offset - shape: (1,), dtype: float64\n",
      "      [Dataset] raw_value - shape: (1,), dtype: float64\n",
      "    [Group] i13-lt-c01-ex-fent_v-mt_u\n",
      "      [Dataset] controller_record - shape: (), dtype: |S25\n",
      "      [Dataset] offset - shape: (1,), dtype: float64\n",
      "      [Dataset] raw_value - shape: (1,), dtype: float64\n",
      "  [Group] User\n",
      "    [Dataset] affiliation - shape: (), dtype: |S18\n",
      "    [Dataset] mail - shape: (), dtype: |S35\n",
      "    [Dataset] name - shape: (), dtype: |S13\n",
      "    [Dataset] role - shape: (), dtype: |S8\n",
      "  [Dataset] configuration - shape: (), dtype: |S20371\n",
      "  [Dataset] duration - shape: (), dtype: |S7\n",
      "  [Dataset] end_time - shape: (), dtype: |S19\n",
      "  [Dataset] experiment_identifier - shape: (), dtype: |S12\n",
      "  [Group] scan_data\n",
      "    [Dataset] epoch - shape: (5600,), dtype: float64\n",
      "    [Dataset] epoch_sampling_time - shape: (5600,), dtype: float64\n",
      "    [Dataset] orca_image - shape: (5600, 2048, 2048), dtype: uint16\n",
      "    [Dataset] orca_timestamp - shape: (5600,), dtype: float64\n",
      "    [Dataset] tomo1Rz1 - shape: (5600,), dtype: float64\n",
      "    [Dataset] tomo1Rz2 - shape: (5600,), dtype: float64\n",
      "    [Dataset] txm1Rz1 - shape: (5600,), dtype: float64\n",
      "  [Dataset] scan_log - shape: (), dtype: |S287166\n",
      "  [Dataset] title - shape: (), dtype: |S35\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def explore_hdf5(file, group=None, level=0):\n",
    "    \"\"\"\n",
    "    Explore l'architecture d'un fichier HDF5.\n",
    "    \n",
    "    Parameters:\n",
    "        file: Le fichier HDF5 ouvert (avec h5py.File).\n",
    "        group: Le groupe ou chemin à explorer (None pour explorer tout le fichier).\n",
    "        level: Niveau actuel de la profondeur dans l'arborescence (pour l'indentation).\n",
    "    \"\"\"\n",
    "    if group is None:\n",
    "        group = file\n",
    "\n",
    "    for key in group:\n",
    "        item = group[key]\n",
    "        indent = \"  \" * level  # Indentation pour montrer la hiérarchie\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"{indent}[Group] {key}\")\n",
    "            explore_hdf5(file, group=item, level=level + 1)  # Recurse dans le groupe\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"{indent}[Dataset] {key} - shape: {item.shape}, dtype: {item.dtype}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "\n",
    "with h5py.File(file, \"r\") as h5file:\n",
    "    print(\"Architecture du fichier HDF5 :\")\n",
    "    explore_hdf5(h5file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des datasets avec 3 dimensions...\n",
      "Datasets 3D trouvés :\n",
      "  - Chemin : /flyscan_0001/scan_data/orca_image, Shape : (5600, 2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def find_datasets_with_dim_3(file, group=None, path=\"\", results=None):\n",
    "    \"\"\"\n",
    "    Trouve les chemins des datasets avec 3 dimensions dans un fichier HDF5.\n",
    "\n",
    "    Parameters:\n",
    "        file: Le fichier HDF5 ouvert (avec h5py.File).\n",
    "        group: Le groupe ou chemin à explorer (None pour commencer au root).\n",
    "        path: Chemin actuel dans l'arborescence.\n",
    "        results: Liste pour stocker les chemins des datasets trouvés.\n",
    "\n",
    "    Returns:\n",
    "        Liste des chemins des datasets avec 3 dimensions.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        results = []\n",
    "\n",
    "    if group is None:\n",
    "        group = file\n",
    "\n",
    "    for key in group:\n",
    "        item = group[key]\n",
    "        current_path = f\"{path}/{key}\"\n",
    "        if isinstance(item, h5py.Group):\n",
    "            # Recurse dans les sous-groupes\n",
    "            find_datasets_with_dim_3(file, group=item, path=current_path, results=results)\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            # Vérifie si le dataset a 3 dimensions\n",
    "            if len(item.shape) == 3:\n",
    "                results.append((current_path, item.shape))  # Ajoute le chemin et la forme\n",
    "    return results\n",
    "\n",
    "# Exemple d'utilisation\n",
    "with h5py.File(file, \"r\") as h5file:\n",
    "    print(\"Recherche des datasets avec 3 dimensions...\")\n",
    "    datasets_3d = find_datasets_with_dim_3(h5file)\n",
    "    \n",
    "    if datasets_3d:\n",
    "        print(\"Datasets 3D trouvés :\")\n",
    "        for path, shape in datasets_3d:\n",
    "            print(f\"  - Chemin : {path}, Shape : {shape}\")\n",
    "    else:\n",
    "        print(\"Aucun dataset avec 3 dimensions trouvé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/flyscan_0001/scan_data/orca_image', (5600, 2048, 2048))]\n"
     ]
    }
   ],
   "source": [
    "print(datasets_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = []\n",
    "slice_number = 150\n",
    "\n",
    "with h5py.File(file, \"r\") as f:\n",
    "    for path, shape in datasets_3d:\n",
    "        image = np.array(f[path][:, slice_number, :])\n",
    "        name_image = (path.strip(\"/\").split(\"/\"))[-1]\n",
    "        layer.append((image, {\"name\": name_image, \"metadata\":{\"slice_number\": slice_number, \"paths\": file, \"key\": path, \"original_shape\": shape}}, \"image\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 7861,  7492,  7364, ..., 17818, 17644, 17738],\n",
      "       [ 7666,  7419,  7622, ..., 17964, 17827, 17350],\n",
      "       [ 7548,  7750,  7512, ..., 17561, 17973, 17698],\n",
      "       ...,\n",
      "       [ 7744,  7308,  7327, ..., 17597, 17827, 17698],\n",
      "       [ 7353,  7419,  7253, ..., 17781, 17754, 17698],\n",
      "       [ 7314,  7528,  7475, ..., 17892, 17864, 17853]], dtype=uint16), {'name': 'orca_image', 'metadata': {'slice_number': 150, 'paths': 'D:\\\\Soleil\\\\024_wt1_450mA_u18gap10p6_Au26um_orca_2x_3p075um_d1ts1037_50ms_5400proj\\\\024_wt1_450mA_u18gap10p6_Au26um_orca_2x_3p075um_d1ts1037_50ms_5400proj.nxs', 'key': '/flyscan_0001/scan_data/orca_image', 'shape': (5600, 2048, 2048)}}, 'image')]\n"
     ]
    }
   ],
   "source": [
    "print(layer)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(slice(None, None, None), 150, slice(None, None, None))\n"
     ]
    }
   ],
   "source": [
    "dim = 1\n",
    "\n",
    "index = [slice(None)] * 3  # Crée une liste de slices pour 3 dimensions\n",
    "index[dim] = slice_number  # Remplace le slice correspondant à 'dim'\n",
    "\n",
    "print(tuple(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nxs(paths, stack=False):\n",
    "    \"\"\"\n",
    "    Lit les données des fichiers HDF5/NXS avec une seule sélection de slice et dimension,\n",
    "    puis organise les layers pour chaque dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : list[str] | str\n",
    "        Chemins vers les fichiers à traiter.\n",
    "    stack : bool\n",
    "        Si True, empile les données des différents fichiers dans une seule couche.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Une liste contenant des tuples pour chaque dataset, où chaque dataset est empilé\n",
    "        sur tous les chemins (profondeur = nombre de chemins).\n",
    "    \"\"\"\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "\n",
    "    # Sélection unique des slices et dimensions\n",
    "    print(\"Sélection de la slice et de la dimension pour tous les fichiers.\")\n",
    "    slices_info = display_and_select_slices_in_single_window(paths[0])\n",
    "\n",
    "    # Préparation des layers\n",
    "    dataset_layers = {}  # Stockera les images par dataset\n",
    "\n",
    "    for path in paths:\n",
    "        with h5py.File(path, \"r\") as h5file:\n",
    "            print(f\"Traitement du fichier : {path}\")\n",
    "            datasets_3d = find_datasets_with_dim_3(h5file)\n",
    "\n",
    "            if not datasets_3d:\n",
    "                print(f\"Aucun dataset 3D trouvé dans {path}.\")\n",
    "                continue\n",
    "\n",
    "            for keys, shape in datasets_3d:\n",
    "                if keys in slices_info:\n",
    "                    slice_info = slices_info[keys]\n",
    "                    slice_number = slice_info[\"slice\"]\n",
    "                    dim = slice_info[\"dimension\"]\n",
    "\n",
    "                    # Crée un index pour extraire la slice dans la dimension spécifiée\n",
    "                    index = [slice(None)] * 3\n",
    "                    index[dim] = slice_number\n",
    "                    # Charger les données selon l'index calculé\n",
    "                    data = np.array(h5file[keys][tuple(index)])\n",
    "                    data = np.squeeze(data)  # Supprimer la dimension de la slice\n",
    "\n",
    "                    # Ajouter les données à l'ensemble correspondant au dataset\n",
    "                    if keys not in dataset_layers:\n",
    "                        dataset_layers[keys] = []\n",
    "\n",
    "                    dataset_layers[keys].append(data)\n",
    "\n",
    "    # Construire les layers en empilant les données pour chaque dataset\n",
    "    layers = []\n",
    "    for dataset_key, images in dataset_layers.items():\n",
    "        # Empile les images de tous les chemins pour ce dataset\n",
    "        stacked_images = np.stack(images, axis=0) if stack else images\n",
    "\n",
    "        # Construire les métadonnées\n",
    "        name_image = (dataset_key.strip(\"/\").split(\"/\"))[-1]\n",
    "        metadata = {\n",
    "            \"slice_number\": slice_number,\n",
    "            \"dimension\": dim,\n",
    "            \"paths\": paths,\n",
    "            \"dataset_key\": dataset_key,\n",
    "        }\n",
    "\n",
    "        layers.append((stacked_images, {\"name\": name_image, \"metadata\": metadata}, \"image\"))\n",
    "\n",
    "    return layers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
